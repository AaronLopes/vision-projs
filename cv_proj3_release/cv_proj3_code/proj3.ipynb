{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YA15tANJ0ciI"
   },
   "source": [
    "# CS x476 - Fall 2021\n",
    "# Project 3 : Scene Recognition with Deep Learning\n",
    "\n",
    "## Brief\n",
    "* Due: Monday, October 25, 2021, 11:59PM\n",
    "\n",
    "* Hand-in: through Gradescope\n",
    "* Required files:\n",
    "  * `<your_gt_username>.zip`\n",
    "  * `<your_gt_username>.pdf`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-TUO3hQhZ-2"
   },
   "source": [
    "## Outline\n",
    "In this project, we will use *convolutional neural nets* to classify images into different scenes.\n",
    "\n",
    "Basic learning objectives of this project:\n",
    "1. Construct the fundamental pipeline for performing deep learning using PyTorch;\n",
    "2. Understand the concepts behind different layers, optimizers.\n",
    "3. Experiment with different models and observe the performance.\n",
    "\n",
    "\n",
    "## Compute Requirements\n",
    "\n",
    "This project is doable without a GPU, but a GPU makes the training and testing portions of this project in Parts 4-6, and the Extra Credit much faster. To use GPU for these parts, you can try out Google Colab to run this notebook. Please read and follow the below instructions carefully.\n",
    "\n",
    "## Colab Instructions\n",
    "These are the steps we follow if you would like to use Colab:\n",
    "1. Upload this notebook to Google Drive and open it in Colab.\n",
    "2. Zip the project folder (cv_proj3_release) on your machine.\n",
    "3. Now, we will upload the zip to Colab runtime environment. To do this, click on the `Files` icon on the left pane. This should show the Files that are present in the runtime.\n",
    "4. Upload the zip folder to the runtime env by clicking on the `Upload` icon present in the top ribbon of this `Files` pane (the one with little upwards-pointing arrow). You should see the zip files in the  `Files` tab now.\n",
    "5. Unzip the uploaded zip using ```!unzip -qq cv_proj3_release.zip -d ./``` (cell with this command is given below)\n",
    "5. After unzipping, cd into the folder using ```%cd cv_proj3_release.zip```. (cell with this command is given below)\n",
    "6. **Go to Edit selection tab, click on notebook settings and set accelerator to GPU.**\n",
    "7. Colab should have most of the dependencies already installed. If you need to install any dependencies (when you encounter import error), you could add code blcks in Colab and enter ```!pip install <package_name>```. \n",
    "8. Remember, if you are using the free version of Colab, the runtime might only be valid for a limited amount of time if you make no changes. You will lose the uploaded files and runtime environment if the runtime is recycled by Colab. Refresh or run cells often if you need the runtime to be saved.\n",
    "9. **Please complete the coding part first locally and test whether the code basically works before uploading to Colab. Editing .py on Colab runtime is rather inconvenient and doesnt save them locally. You should use Colab only as a platform with GPU to test your model.**\n",
    "\n",
    "Tips: If you don't see changes in the Files tab, click on the file icon on the leftmost bar once more to refresh.\n",
    "\n",
    "***Please note that we only intend for you to use Colab only to test your code in student_code.py. You must be sure to develop your student_code.py locally and make sure all the functions work as desired and THEN upload to Colab to achieve faster training and testing results. IF YOU ATTEMPT TO EDIT student_code.py IN COLAB AFTER UPLOADING IT INTO RUNTIME AND RUNTIME DISCONNECTS/EXPIRES, YOU WILL LOSE THE EDITS THAT YOU MADE TO YOUR student_code.py. Please only use Colab to test your student_code.py. If you need to make any edits to student code after uploading it to Colab, you will have to make the edits locally and follow the instructions above to re-upload the project into Colab Runtime.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFVeSYKzTdjK"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8QRsVzCGTdjB",
    "outputId": "4564f8dc-6d01-426f-a291-d0fbb889c008"
   },
   "outputs": [],
   "source": [
    "# To use on colab, uncomment the following lines\n",
    "#!unzip -qq cv_proj3_release.zip -d ./\n",
    "#%cd cv_proj3_release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_colab_paths = False # switch to True if you're using colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mA7pA3FWhZ_I"
   },
   "source": [
    "## Dataset\n",
    "The dataset is in the ```data``` folder. It has two subfolders: ```train``` and ```test```. Go through any of the folder and you will find the folders with scene names like *bedroom*, *forest*, *office*. These are the 15 scenes that we want our model to predict given an image. You can look into folder for each scene to find multiple images. All this data is labelled data provided to you for training and testing your model. You can look at `dataset.png` present in root folder to get an idea of the dataset. \n",
    "\n",
    "**Let's start coding now!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DikW12-RhZ_J"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "g1dqr6qSBpE2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cv_proj3_code.trainer import Trainer\n",
    "from cv_proj3_code.student_code import (get_optimizer, SimpleNet, SimpleNetDropout, MyAlexNet, \n",
    "ImageLoader, get_fundamental_transforms, get_data_augmentation_transforms)\n",
    "from cv_proj3_code.utils import compute_mean_and_std, visualize\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "CG3e0869TdjS"
   },
   "outputs": [],
   "source": [
    "from cv_proj3_code.cv_proj3_unit_tests.test_base import verify\n",
    "from cv_proj3_code.cv_proj3_unit_tests.test_image_loader import test_dataset_length, test_unique_vals, test_class_values, test_load_img_from_path\n",
    "from cv_proj3_code.cv_proj3_unit_tests.test_data_transforms import test_fundamental_transforms\n",
    "from cv_proj3_code.cv_proj3_unit_tests.test_dl_utils import test_predict_labels, test_compute_loss\n",
    "from cv_proj3_code.cv_proj3_unit_tests.test_simple_net import test_simple_net\n",
    "from cv_proj3_code.cv_proj3_unit_tests.test_simple_net_dropout import test_simple_net_dropout\n",
    "from cv_proj3_code.cv_proj3_unit_tests.test_my_alexnet import test_my_alexnet\n",
    "from cv_proj3_code.cv_proj3_unit_tests.test_checkpoints import test_simple_net_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GjE0jIc5BpFN",
    "outputId": "d19ab856-8e32-40e3-c10f-c84645d6ad73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: False\n"
     ]
    }
   ],
   "source": [
    "is_cuda = True\n",
    "is_cuda = is_cuda and torch.cuda.is_available() # will turn off cuda if the machine doesnt have a GPU\n",
    "print(\"GPU available:\", is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IKRS4gvZTdji"
   },
   "outputs": [],
   "source": [
    "data_base_path = '../data/' if not use_colab_paths else 'data/'\n",
    "model_base_path = '../model_checkpoints/' if not use_colab_paths else 'model_checkpoints/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1QoVOsGwhZ_j",
    "outputId": "aed9e584-8fb2-40cb-9489-0cedb5a3c1a7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'test', 'train']\n",
      "['simple_net', 'quantized_alexnet', 'simple_net_dropout', 'alexnet']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(data_base_path))\n",
    "print(os.listdir(model_base_path))\n",
    "\n",
    "# TODO: check that these outputs are as per expectation. It will save a lot of time in debugging issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nulEn5fzTdjs"
   },
   "source": [
    "To train a network in PyTorch, we need 4 components:\n",
    "1. **Dataset** - an object which can load the data and labels given an index.\n",
    "2. **Model** - an object that contains the network architecture definition.\n",
    "3. **Loss function** - a function that measures how far the network output is from the ground truth label.\n",
    "4. **Optimizer** - an object that optimizes the network parameters to reduce the loss value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B027mwlKTdjt"
   },
   "source": [
    "## 1 Datasets\n",
    "One crucial aspect of deep learning is to perform data preprocessing. In DL, we usually *normalize* the dataset and perform some *transformations* on them. The transformations can either help the inputs be compatible with the model (say our model only works on 500x500 images and we need all input to be cropped/scaled to this size) or help in data-augmentation to improve performance (more on this later).\n",
    "\n",
    "\n",
    "### 1.1 Compute mean and standard deviation of the dataset\n",
    "In any machine learning task it is a good practice to normalize the features to a fixed range. In this project we are going to \"zero-center\" and \"normalize\" the dataset so that each entry has zero mean and the overall standard deviation is 1. \n",
    "\n",
    "We have provided util function `compute_mean_and_std()` in `utils.py` to compute the **mean** and **standard deviation** of both training and validation data. Make sure to understand the logic behind them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "vWA_2UbjBpFd"
   },
   "outputs": [],
   "source": [
    "dataset_mean, dataset_std = compute_mean_and_std(data_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xixFr8CDBpFn",
    "outputId": "b9c34dca-34d6-4af5-f00f-341acf3cac34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset mean = [0.45547487], standard deviation = [0.25316328]\n"
     ]
    }
   ],
   "source": [
    "print('Dataset mean = {}, standard deviation = {}'.format(dataset_mean, dataset_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2TeGbrQBpFu"
   },
   "source": [
    "### 1.2 ImageLoader\n",
    "\n",
    "Now let's create the **Datasets** object to be used later during training. Remember back in Hybrid Images Project, we have initialized such a class to load 5 images? Here the task is similar: we have to load each image as well as its classification ground-truth label. The essence is to retrieve the paths to all the images required, and be able to provide the **path** and the **class id** when given an index.\n",
    "\n",
    "**TODO 1:** Go through the code of `ImageLoader` in `student_code.py`, run the following code cell to answer question in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "THRvAvluXFcS"
   },
   "outputs": [],
   "source": [
    "# Initialize a new image loader instance of the 'test' dataset\n",
    "# For train dataset, use 'train' for split\n",
    "input_size = (64, 64)\n",
    "image_loader = ImageLoader(data_base_path, \n",
    "                           split='test',\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p0aTIbfyf6ex",
    "outputId": "6feee43d-a04d-4dd7-8def-430d53f76ad5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset:  1500\n"
     ]
    }
   ],
   "source": [
    "# Length of the dataset, returned by the __len__() function in ImageLoader class\n",
    "print(\"Length of dataset: \", len(image_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oHqc-Igof6ey",
    "outputId": "64014bc4-db26-4e7c-9772-92b67c593953"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes:  {'forest': 0, 'bedroom': 1, 'office': 2, 'highway': 3, 'coast': 4, 'insidecity': 5, 'tallbuilding': 6, 'industrial': 7, 'street': 8, 'livingroom': 9, 'suburb': 10, 'mountain': 11, 'kitchen': 12, 'opencountry': 13, 'store': 14}\n"
     ]
    }
   ],
   "source": [
    "# Classes in the dataset, returned by get_classes() function in ImageLoader class\n",
    "print(\"Classes: \", image_loader.get_classes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the data type of the image and label returned from the image_loader. Is this the data type we want? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data type of sample_image : <class 'PIL.Image.Image'>\n",
      "sample label : 0\n"
     ]
    }
   ],
   "source": [
    "sample_image, sample_label = next(iter(image_loader))\n",
    "print(\"data type of sample_image : {}\".format(type(sample_image)))\n",
    "print(\"sample label : {}\".format(sample_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1B4thhGf6ez"
   },
   "source": [
    "**Please answer the questions related to Part 1 in your report after running through this section.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIq75D2CTdkC"
   },
   "source": [
    "### 1.3 Data transforms\n",
    "In this section, we will construct some fundamental transforms to process images into torch tensors, which we can provide as input to our model.\n",
    "\n",
    "1. Resize the input image to the desired shape;\n",
    "2. Convert it to a tensor;\n",
    "3. Normalize them based on the computed mean and standard deviation.\n",
    "\n",
    "**TODO 2:** For this part, complete the function `get_fundamental_transforms()` in `student_code.py` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZoAaL11nTdkD",
    "outputId": "00166f6e-9bfd-42e1-db9c-4ad900995b54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing your fundamental data transforms:  \u001b[32m\"Correct\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing your fundamental data transforms: \", verify(test_fundamental_transforms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that you have implemented fundamental transform, lets pass these as tranforms and reinstantiate our image_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (64, 64)\n",
    "image_loader = ImageLoader(data_base_path, \n",
    "                            split='test', \n",
    "                           transform=get_fundamental_transforms(input_size, dataset_mean, dataset_std)\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oh6vcdbCf6ey",
    "outputId": "83295669-8cda-4bad-866f-9df0f336c6b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image shape =  torch.Size([1, 64, 64])\n",
      "Label index =  0\n"
     ]
    }
   ],
   "source": [
    "# Create an iterator and get the 0th item, returned by __getitem__() function in ImageLoader class\n",
    "sample_image, sample_label = next(iter(image_loader))\n",
    "print('Input image shape = ', sample_image.shape)\n",
    "print('Label index = ', sample_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "tJAd72_uf6ey",
    "outputId": "a6c28f40-aa7e-42b8-f8fe-30827d32186b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD3CAYAAADmIkO7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAr/ElEQVR4nO2de+xl13XXv8vv+Dmxx+N5z7i1jZ0AJUWVW9SQVk3aBjApIoWmKYTQigoJiqCEhELbFFJCUSReorRCDaF1H7iEVEAxTQpOapU0TmzHr9iN43jGM54Ze/wYZ/yK48nhj3Pv/X3O13evuR7/Eu8x6ytZPnfOOfusvc/ev/P97rXW3jEMgwqFQn845eU2oFAoLEcNzkKhU9TgLBQ6RQ3OQqFT1OAsFDpFDc5CoVO8IgdnRLw3Iq49wXvviojvWF+L1h8RcXFE/GFEnPVy27IqXsx7iYgPRcT7TvA5J3zviSIiboqI165nmes6OCPi2yPi/0bEExHxWET8fkR8y3o+42uNYRheOwzDx19uO1bAeyT9x2EYnp39QXly9t+xiHgWv3/i62FMROyOiCEiTvt6PO/lQET83Yg4NOvfH4yIM3H6A5L+yXo+b90GZ0ScL+l/SPq3ki6UtE3Sz0j68no9ozBi1ineIelaafEH5dxhGM6VdKOkvzX/PQzDP1uxzFfsoFoPRMT3aPyD+F2Sdkv6Bo39e47/Juk7I2LLej1zPb+cV0jSMAy/PgzDsWEYnhmG4aPDMNwuSRHxjRHxfyLi0Yh4JCJ+NSI2zG+OiD0R8a6IuD0inoqIX4qISyLi+og4GhG/GxGvnl07/yv9NyLiQEQcjIgfbxkWEd86+6IfiYjbMto6s+ONs+P3RsRvRsS1MxvuiIgrIuIfRsTDEbEvIr4b974zIu6eXfvFiPhRK/sfzGw9EBE/MqvDZbNzZ0bEByLigYh4KCJ+ISJe1TDzaklHhmHYn72QFdv83RFxu6SnIuK0iPirEbF3ds9PWnucEhHviYj7Zuevi4gLZ8X93uz/R2Zf7G/LbJuV95v4Ev3eElq4MSI+NmvPT0TELtx75ezcYzHS+790vOe9RLxD0i8Nw3DXMAyPS/qnkv7a/OQwDM9KulnSdy+//cVjPQfn5yUdi4j/FBFvng8kICS9X9JWSVdJ2iHpvXbNX5T0Jo0D/RpJ10v6CUkbZ7b+mF3/nZIu19gg75l3oslDI7ZJ+m1J79P4Rf/7kj4cERevWK9rJP2KpFdLulXS78xs2aaRxvwirn1Y0p+TdL6kd0r6lxHxzTM7vlfS35P0RkmXSXqDPefnZvX+E7Pz2yT9VMOmPybpD1ewfZU2f5ukPytpw+z5Py/p7ZK2SLpgZsccPybp+2a2b5X0uKR/Nzv3p2f/3zD7Yn9yBfuu1/j+Nkm6RdKv2vm3axwEGyV9dn4+Is6R9DFJvza7922Sfn7J4H4BYpReR5L/vr1x62sl3Ybft0m6JCIuwr/dLembjmfDyhiGYd3+09gBPiRpv6TnNX7qL2lc+32SbsXvPZLejt8flvTv8ftvS/qt2fFuSYOkK3H+X2j8yyaNHfDa2fG7Jf2KPft3JL2jYdceSW9EOR/DuWskPSnp1Nnv82Z2bGiU9VuS/s7s+IOS3o9zl83uvUzjIHpK0jfi/LdJur9R7j+S9BuNcx+X9CMvos3/On7/lKRfx++zJT2H9rhb0nfh/BZJX5F0Gt7JaUn/WLyXJec2zO6/YPb7Q6yjpHMlHdP4B+YvS7rR7v9FST+Ne9+3zn37Pknfi9+nz+zdjX/7WUkfXK9nrqvOGIbhbs0+9RFxpUZN9K8kvS0iNkn6N5Jer7FTn6LxLy/xEI6fWfL7XLt+H473avyiOHZJ+v6IuAb/drqkG45boeU2PTIMwzH81syuIxHxZkk/rfELdIrGzn3H7Jqtkj7TsP3i2bU3R8T830LSqQ2bHtfYhilWbHPasZW/h2F4OiIexfldkj4SEV/Fvx2TdMnxbFli26kaO/P3a6z/vMyNkp5w24ZheDIiHpvZuEvS1RFxBEWeppHhfK3wpEZGNMf8+Cj+7TxJtOkl4WvmShmG4R6Nf8H+6Oyf3q/xL80fH4bhfEk/pLEDvhTswPFOSQeWXLNP45dzA/47ZxiGf/4Snz1BjJM0H9Y4a3fJMAwbJP1PrdXxoKTtDdsf0TjQXwsbLxjGCZ5luF0zjX8crNLmTEua2DjTvKRt+yS92dryrGEYHrRyVsEPSnqLRpp/gcYvr8y+RRtFxLkaZcmBmR2fMDvOHYbhbx7voRHx+libyV723+sbt96lKWX9JkkPDcPAP15XaUp9XxLWc7b2yoj48YjYPvu9Q6MW+IPZJedp/OtzZKYD37UOj/3JiDh7pjXeKek/L7nmWknXRMT3RMSpEXFWRHzH3M51xBmSzpR0WNLzs68oJweuk/TOiLgqIs4W9OQwDF+V9B80atRN0qiVY5whXIabJG2YtWOGF9vm/0VjW/2piDhD42wkB8svSPrZ+cRMjL7Wt8zOHdb49fuG4zyDtn1Z0qMaWcOyWeU/M9OIZ2jUnp8ahmGfRq/AFRHxVyLi9Nl/3xIRVx3vocMw3DiszWQv++/Gxq2/LOmHI+I1s/mUf6zx4yNp8cf5T2rUwuuC9fxyHtU4i/ipiHhK46C8U9J8FvVnJH2zRsry25L+6zo88xOSviDpf0v6wDAMH/ULZi/zLRonlg5r/Kv7Lq0zaxiG4ajGCZPrNFLHH9Souefnr9dIMW+Y2TyfMJm7mt49+/c/iIgvSfpdSX+k8aznNHaMHzqOWS+qzYdhuEujtv8NjV/RoxonueY2/utZnT4aEUc1vuOrZ/c+rZGm/v5sYuVbj2PbL2uUIg9K+pzW/ogTv6ZRJjymseO/ffasoxr/8P2Axi/pIY0TamcuKWNdMAzD/9I4r3HDzO69M9vm+POSPj4MwzL2dkKImZA9qRARuyXdL+n0YRief5nNOSHM/srfKenME6nDbLb5RkmvG4bhmeNdfyKYUckjki4fhuH+r8UzXimIiE9J+uFhGO5crzJfkeF7vSIi/kJEnDGjRT8n6b+f6B+XYRgOD8Nw5XoPzIi4ZiYVztGon+/QOKtbSDAMw9XrOTClGpxfb/yoRmp9n8ZZzuNOYLwMeItGqnhAow/yB4aTkV69AnBS0tpC4f8H1JezUOgUaRDCPffcs/isnn766ZNzX/7yWjz7+eefPzn3qlethYSecsopS4/9N5zv6Tm/rnVPdq1ftyq8PP4mA/HrvvrVNZ/9vn37JuduvfXWxfGhQ4cWx0ePHp1c99hjjy2On3rqqcm5J598cumzzzjjjMl1x44dW3qd29iql1/3/PNTuczfbONnn312ct2pp67FVpxzzjmTc3we+1jG8GiTX+vls260K6vLWWe1s/JYhvcrjhlvb5Z/3XXXLe2o9eUsFDpFDc5CoVOktPbVr15LLCEVkaR77rlncUxaJUlnnrnmC77kkrWwy3PPnUajsUz/7PvzWtetem498PTTT09+k+6sSrd37do1Obd58+bFMemqt+kTTzyxOP7MZz4zOXfbbWsRY88991zT3lVBe0mFJem009a6jNeZNI7U0mknf3v5X/nKV5aW4dSyRU/9voyuZu/s7LPPXhw7XWW7PvroWvQe5ZyXwXZb9nsZ6stZKHSKGpyFQqeowVkodIqU+HIa2vXLkSNHFsfk1pL0yCOPLI63b19L/qAWlU5MP74YXXkiARZ+DzXKF7/4xcm5HTvWsr5a7iNparNrIJ5je3ubbtiwYXF8++23T865bpvD25vazOvJ39R9jsytQL2buVIyLcm5BtbL2y17t5kriP2YbeptRbj+Z31YZ3c3fulLX1ocX3TRRZNz2fPmqC9nodApanAWCp0ipbWkGO7aIG359Kc/PS0U08SMHrr66qsn12X0g7+ziJ4WpXObMzqcRfeQ0nz2s5+dnGMEzpVXXtksn6BLRJrSVx57e5AGvelNb5qc27lz5+L4k59cW1fLn0V7n3lmmszCaBwee/s6vSRIURnRlPUdj3Yitc+uI1wC0H6nmqw37brgggsm17EfeBucd955S895VBfbKouEaqG+nIVCp6jBWSh0ihqchUKnWHlpTNcadCs41+Zvhqu5BmJ4oOtKTkNTy3jYE+3wMjxUrlUGp8MdX/jCFxbHzCCRpnqGus81Cn/fddddk3Nbtqyt3n/55Zcvjt39wDa98MILJ+de97rXLY4///nPN+1461vfujg+cGC61A2fR73I+kvSLbfcsjh2rcf3RA3nbjhqQu9XrhHnyDKaPCuKbeVa9fDhw4vjVV1Grpldr8+R6XFH1ufmqC9nodApanAWCp0ipbWcTnYKwAwTpw6kJkwufvjhhyfXkdY6BSP9oBvBKd29997bLGPjxo2LYyYyM7JHku64447FsdMq0lCPFGE2yKZNmxbHWTQL7ZCm9J1t+sADD0yuY3u424ZtQJsef3y6uDupa0btDx482LSX8D7BtmP0jWel8D25S4F0lfc5zc/cQiyfdfFy+J5cmrFMd4OwXTO3E6mxl79Kwn99OQuFTlGDs1DoFCmtZQD7DTdM9/3Zs2fP4tg/2aQcpIxveMN01zvSIg/mJp286qq1VfY9YZt48MEHJ7/371/bvpIzrU5PSQt9ZpG0yGftSNMZmePIgsBvuummxfFDD63tmcTEAmlKO72e99133+KYyb9exkc+8pHFMaNcpKk0uf/++5f+u/TChGKCtI52uFTge/fyWU/SSZcKfBfuBWhdJ7Ujc7xPZLO1rWRxp++Ej5FVUF/OQqFT1OAsFDpFDc5CoVOkmpOajQt1SdPIHNcN5NfUFzfffPPkuq1bty6Oqb0k6XOf+9zimDqNNvnvO++cblVBzUX9zGNpqllcc/LZrm3oavIyCbaP6y+Wf+ONa7vPubajbnWXFPUS68xIH2mqu92dRI3I8rKFqDyTg7qQLiKPJMo0J8H2zdqNz5KmbefuHp5jW7lrg/Mm/t5ZJtvH9Szt8rmGcqUUCicxanAWCp3ieNsxLI4zFwOjY6TpJ/vuu+9eHLu7gfd5IDZ/Z+u0MHKGrgi3mcHRrJeUUzfSkWwNV1JBd/fQfl8zh/Qpc9vQDm8DyggeO82i/R5V01pDyN0DpK6+Dg4pKu3wCBtGbjn1po18f5kLx8Ey3EXCiB6ec9cS361Tb7Ydo9z8vbeeJb0wWH8Z6stZKHSKGpyFQqeowVkodIpUczLR1jUnXQfO1xm1T/3p4WTXX3/94tjdA62QLJ+WzxJcqZ3I/12/cMrbtR51lWsKlp9tBUdd6bqVmo7PppaWpvVkxoc01UDUjj59n7l7WnvTuN7K9Ch1Zqbjs3fWWhPWy8v0M/uZu1J4jmV48jNt9PfO+9iXsi0ivd/6u1mG+nIWCp2iBmeh0ClSWssoHac9pJ2kjNKUZvDz7W4E0oMsUoTU0ukA6YjT65YbxOkHqQ6nxqVpvZ0itabsnSLRdZCVka2Bmq3PyzJ5nbcH6+LlMwKMND/biiCzKUs0ph2ZS6G1y7WUU1e/liAVzxKl2Xbe90mx2TfddcU28B3Ns7abo76chUKnqMFZKHSKlNZmM5Ct4HY/Rxrhs2qkTB5ETdq8KqXzyBmWTyrrtDNbvp/nnA6TxvBZ2bo4Xhe2MenTiQZRs24ewcNn+65XfGfZLCmpmtvRih7ywHRSWa9na8fqLCHB3yfr6RSXzyN19dlUztD6O2ud83GQLX/JGfwW6stZKHSKGpyFQqeowVkodIpUc2YLFlEbeMQK3SLk3R4hRD3j0/7MZKC+8Gl5lu+RP3wej91e2uHnWE/XX9QYHtFDUEu6O4lalef8Oj7bszyo5dkertOo9VzzUBeyXt4e1FhZxFHLJmmqTVfd3dzLyCKVaKPPlbCPcH6B7i5p2nYerXbxxRcvjjN3D9vYFyvw5y1DfTkLhU5Rg7NQ6BQprSUV9EgO0h2fUue0dGv5ez+X7UBMCuM0iJQm234g+3fu9OVT+3y2u4JaQfFOO0l5nWa1Iky8PUjHnNozYZl02Okeo5/cdUXaTHrm7phWhI2XQfuz9+7t3dplzN0ZpOH+Xtg3Pbqn5fLy98L73KbW2sn+7yzf12xaZR3b+nIWCp2iBmeh0ClqcBYKnWLl8L1sytu5Nvk7dUgWzpTtcEydkOkX11it6XZ3U/C3u0SobVxLUsNt27ZtcfxiFufidLu7PlrP4hqzDr4Ld/2wDLeRdfPQPoL9wN873xPb3uvFuQx36bTCA32eIHNPsf95e7NudAX54nBsD66vLE3bMdtGkPMEbmPmppyjvpyFQqeowVkodIqU1vKz7Em3/Cw71SSlIYVhZIWUb41HCpwlbNMOz34grXNqQtB+z05gdIi7akjx6HLwMth2TuP4m3a464Bt5W1AVwpp+eWXXz65ju/Fo15I/3jsdrAMdz+QvrIu/m6zxHe+d5bhNJDPzui1R+awTLoH3cbMJcU2yLJLSL2d2rfWbCLqy1kodIoanIVCp0hpbTZLys+0z8KSEjAI3oPbSce8/GwHa4Kzb57syggNHjvt5DkPzmcbOMUj/WMbZLspO53krCBnYb09siRnPo92OM2nHdkyjqSMHn1DGueJBmzXLBEgK4P3USp4H2NdfP0pRj/t3Llzco7lcEfwrC5OXVvt7cjWKKpdxgqFkxg1OAuFTlGDs1DoFKnmpOvA3SDUPe5ioLakdswiT7yMlqb1LAlydz/H+8j/PdE1S9zlOXdhUHMyasT1BfVjti0f4dE9tMvP7dq1a3F86NChpeX5s7ItAPks11TZrtHUeoyW8TrSheEan9qX97nu41yARwFRZ2bRYJs3b15anjTtf55RQrsyl2K2deUqqC9nodApanAWCp0ipbXZGiukQR79QIqwe/fuxbFPV5MGOCVoBQZnkRU+pc7fWXI4r3P3Q0ZHWA7LuPDCCyfXsd7uSmE9r7jiisWx1/OBBx5YHLtLijZu3759cZxFda2yy9UyUB44RWcbsM7eP1hG5qphPf062u+Ul+ecerNdL7300sWxtxXdfN4XScUpMbKEhyyxo4X6chYKnaIGZ6HQKWpwFgqdItWc2Z4WdB34dHhLj7q7pLUglDSdUidfd/3iU+UEdQpdLl4XumDcTcEy3CXgGnoO36WbNmYhgNQ2Ht6VZXK0EuG9DGog13C0g+3jdczam2UwDM81WyvzRGpv9+jXMbzO19ZlJopryVVdRmzTbI8f9pdscbhypRQKryDU4CwUOkVKa0mLfN0aRoP4btCMDiH98E87aYvTJ1IEXpe5BzyKqRUR4zSFLgCntfydUTwe+zYFdM84xdu0adPimFTQ3QPZmjOtna29vVlGtv4vkW2lkK0dRTeIu7g8K6hVPts0W5vWqTZ3kXZq3yrTpRnptbeV95E5vL3pWnE5UztbFwonMWpwFgqdogZnodApUs1Jbu3cvbVQktRenCtb7cDDrKhxuVqAh0hlWox2ZXtfEG4HtYFrG+oeXuf1ZCiYa0mG82XhaoSHGLYyOfydUftlqykQWca+14XvnW4znyeg/V4G+xzbwK/j/jbeBxge6PVifbKVPvius7WS2Ye9rVr7ski1V0qhcFKjBmeh0ClSWks4lcoW4OI5UoyMHmTIdpcmlcoSg7Mdn0mZfNqfdCRLgCYd9rYhRfcoqVayuE/t013l7Ua7snVUSXkzukdK5+89c6/Rjszlwvb3NmX5lAPuJuN9WfksQ2pvr5HJJXeN8dosA4b91t0xTnOXob6chUKnqMFZKHSKlNbyM+30I5tZXGUHJSnfoamViO00i3Q1i+5hXZx+tJKmpek6M6vOuDkl5eywUzBGy5CC+Y5mtNnlQWsbB69n6x5pSod912uCNnqkD+keaWjWHr47Nikk7c+SCZzmt6KM/Bz7rdNm2uVl8HnsA97e2fpCRWsLhZMYNTgLhU5Rg7NQ6BQra85suzefJqYuJLd2HUVO7lEk1IvZvizZ7sfUUSzPn5VpM64D6/d5feZwDc728b07qEGZleILgbEuHuGUZdUQLNPnBfie9u7duzh2vcXFy7ye1GnUo67js0XaaBfrmdXLXWN8nrcV25s60N8t6+L9oxU1lc0F+D1ZZs7inuNeUSgUXhbU4CwUOkVKa7Pt+0iDsjVWeJ1vl+DT10SL8jod4+8sAJp2uDuDtMWnuEknnb7zHMvw61pJyNKU4nHZf99CL4umyugTwfZ3N1Br2j+LbPG2aq055dSSNNTLIN1rSRu3K3Pd+RrC7BPZjuOZXOKzs4Rq2uXlOwVehvpyFgqdogZnodApanAWCp0i1Zyrrt2ZhSJRiznvpkZx1wGn8Hnd/v37J9dRQ2Tbg1O/uPbNpvaZ2eJ6keVkCdUMh/NQNmoPLvaVbXnnIW+tBb78Opbpa73yWmosd11Rr7uNrBv1lvedLMSQ99Em19LsL2w3KV9bt+U2y7J5vDxm41C7Zwuqed/x/r4M9eUsFDpFDc5CoVOsvG6t04Fst+bWNLpnO5BOZrtG0w6PFOF1TleJbLo9W/+HNmeJ2LzPn0VK6m4c/mZklVNtlu8J2y2q5m1FKuXtzTZetb29Pdj+LN8pI2mh297aUo9bGzqcTpJqerJ1q09nW1C6FHG5sMxeaZogn2UBtVBfzkKhU9TgLBQ6RUprGaVy0UUXTc5xJtc/861du5x+kE56EDlpAGcPnY6xfE/EJjUhBc3oqVNjlu+RIqQ72TpHrLdHkXAmM3sW4ZSoFankUTWt5HNpSvcyucF6ZonjpNBeF7ZBtqsbZzj9uizCJlvXh++Xu5H5rDTtd9nGemdrGWVrMVWydaFwEqMGZ6HQKWpwFgqdItWc5OfOkcn5XXNSU+zevXtx7FPS2U7O1IXUIR7ZQq3g0+Z0OWRJ39m2eYTrUS46lSVsU4txwTAvg9rRt7jIdqVmfbL1hKkl/Z3xN+vpScisS9YnWjtIS7kbgTqZGSWuMan1PHGZutj1Lt8vn+X9j23qLkD2ucxlxP7nfd817jLUl7NQ6BQ1OAuFTrFyhJAn3ZIu+LlWwqxHUJAKOtUkLcpoSmsXbemFroSWHS3Xj5fhLpjW2rpOf7N1d0lDWbeMBrn9XOuVtN/tYJm+SzfbOEtCznbHIsXjs90OlpElUTOSyKklE/W97zCayKk3bcloLevtie/sI5REvm0D35lLkUq2LhROYtTgLBQ6RQ3OQqFTpJpz69ati2Ofoid3zxZA4jS6h64xRMpdEeTo1ECuK7PEWmoD2uQ6hG6EbItBv6/lgvGFy9h27u5pZcu4RmH5XmfaQR2VZZ64dqSOzRYJy7a1Y1vxXBa26WXwN+33ZOVsXVz2zde85jWTc/v27VscZ+4pnnONTxvZb90OamHX+L73yzLUl7NQ6BQ1OAuFTpHSWtIxp538TPtnP1v2n+A5p1mt9WKzaBOPeqH9pFxOP7LdsemayJbQJ1VzikT4FHpr2X+XAGwDL4NJvYxK8TJIE/2dtbZSdDcI2zujv3xP7j5ind0O2khZ5W3PCCp3O7USx6Up7Wf7ZO6eLFE6S/bP1ufN2m5xzXGvKBQKLwtqcBYKnSKltYx4cKqWJS+3kmSzHcKyqJpsJ2RSDJ9RJp2ijU7RSW+cfmS7qfHZpFZO1dgG2bL8LN8D37M1ilpbAmTXOUgnsyUps6iaFsXzd8v7Nm7c2LSDUUGe8MAynHZmsoLtyplzf7eUCj7rfeDAgcVxa8sPqR0gL9UaQoXCSY0anIVCp6jBWSh0ilRzUkdlu/Y6fya/zjRn67oXGJnsSp1Nh1OXMFojKyPL1nCXALU1n+26mPX2DAqWwen71tqoy9BKlPbIHGox14ut5GhfI5fvzDUt+wTb1J/FBHzfoo86kDa5W4ht6nbwnPdbznPwnK+Lyz7t0TytejqyLSnKlVIonMSowVkodIqVaW02Le+uCUZzcOrdKWM27U/XR7bGJ230hNxWMnS2fksWweNUpBVZ5GVk0T2t9vFkAp7Lds7mOa8n6+KUl+sS00bfDStz6bQic1z20OXlLhL2nWxntda79Wdn0UN8tttBm73fMgm8tXaxl5Ftr9FCfTkLhU5Rg7NQ6BQ1OAuFTpFqzlZomZRrMWoRaqosQ8XdGy0t5lPXfLZrrNa6uK63eJ/XhRoo04tMsPYMCk6j+yJQqy4Slu0G3dKLrlvpksr2/2i9P2mq6/1dtHY797mALVu2LI79nbV0t78XPttDAKmT/b7WPieZHa45d+zYsTimey1zq/j4WQX15SwUOkUNzkKhU6S0lp/9jJI6GOWxadOmxbHTMbpLnBKQGpKK+LQ8qY9TMFIO0qxs3aFsi0FvA9I11s2pK+3K3AqkYJ7pQzs8yqjl0nFqnG3VwDLZpl5n1iUrn++PNNbLcLrHfsC28ugsUldu5ed2eL+izXSfeLJ/tgUg72OEl9vIPuH9apXxVF/OQqFT1OAsFDpFSmtXXd7Qg9YZzEx64zNnpE9O41q7UrsdLYohTWmFR7oQrItHotAOp6StaBwvI1tOkjO5rTWP3MZsPSeW4XSMz/aAcyYX870cPHhwch2pW7ZzNt+T14V2ed9h3dhfnJ7yva+6HYg0fTfe54hWsr/UXrLT3zvfC4P9pRcukboM9eUsFDpFDc5CoVPU4CwUOkWqOakNXOdQD/hUNn+Th3vCaUujLHveHNl6q1kUCbWBR87QxiyB2LVTa1s+dzGwnj6lzvI5nZ+t5+rl035GKnl7Hzp0aHHsycVsE2pr16bZ7tjUu7zPo7+ob93GbJ5jVTBTxNuqlWztoDvM+xXdg3wvrn25i/lll102ObdKxFB9OQuFTlGDs1DoFCmtZdSITztzmth31WrtNuU0ohXBI7Wnyt0OUqlsV62M6mTrC2U0q7X2qNNfUmqnqyyT9NfdFKSQ3o6sD9+FR71kO2dRRmSRLVxPJwusp43+LN7nZfBd8L5VJYU0bTtf/4f9Ktuig/3A24B9Lov+YkC+u/KyaK056stZKHSKGpyFQqeowVkodIpUc7b24PBzrhuoN7J9QrI1VnlfFrqWhWBR62XhWCzTz7Ge2X3Z9oB0F7i25t4ddDFkmjNbnIvt5knCLN9BXcW1al1vsQzX53x29t6zTCLe1wrl8zK8/7FfefieP28ObyvqdV/8i/XOQvSYteQLjbVchUR9OQuFTlGDs1DoFCmt5VSz0zbSHacVrSiMbOrdP/OkAVmidBbBw/tIrdwOXpeth+TRLK1l+bP1Sx0sn8dOXWmzvwtSb66p6nXJdqUmbc7s5Xvy5GK6cViXbOvHLEqHNnoCO9sno7Ue8UUayuv27t3bvM5tdIo6x7Zt2ya/+Z5cfnnbLUN9OQuFTlGDs1DoFCmt5WfZZ2tJfZw6kK6SmngkB2f+/BwjKnxWkCAd87V1WjNi2ZYLfk8WfN1Kts62nfDZw1ZQvD+XlMkpKX+TLmVbHThFb21r4XZwdtnPMeqF5XsEGemk01XazwBzD8BfNbndkzLYb++9997FsUfw8B06LW/tQO5jJFskIOvTc9SXs1DoFDU4C4VOUYOzUOgUqeYkz/cdjqnbfJqYWoc6JMtK8TKoWVieayXqXS/Dp9jncG2QJVvzWndhtDJnfPuBbFsBapZWpI/bmC1WRk2VbXXYaht/lmvwLJui1VZ+D7WY14XPo/3eHuxL2S7RHpHF98v35+4jakLv+5zb4H1exkMPPbQ49nfRilQi6stZKHSKGpyFQqdIaS0pgLs6SEmd7pHi0T3g08e8z6kDg9Z37ty5OL7//vsn12VL+6+6nURGBVvT5tJ0up0SwKf9b7nllsWx0yxey2c7RcoiaVhP0v4sIssjrVpRUt5u/J1RXNrvMoJt6u3Ba7m2q+8kxvbw/pfZT8rO9+nvvRUFJLXb0alqFh2XbQkyR305C4VOUYOzUOgUNTgLhU6Ras5sMSdqCt+jpBWW55H41BRZaB+5u7spWtvfuV2csvek41VDwfw+ai6ec83JMDQvv7V2ry9ClmkbannqKLejFaInTbUf31O2JWKWwZPtc5JtU9hK4nft2ApZlKbhgpmmzRYQ43yCtzcTsTkOMnePl1+as1A4iVGDs1DoFCmtbe0CLE0/4U5NSDNWXUsmS+bO3CWkC04VGG3i9hOkgpnLwl0pjBQh7fJMC9IsL582ZlE7pGdO1Vhvlp9tg+BlkOJllCtbN4nvidd5nVlPp/l0BZGCetuQ1jp9533uomvJMa8z+1kWgcTrMvqeZRK1UF/OQqFT1OAsFDpFSmszOkYal+3ale1KlS07SUrGZzsd46yjz6rx2hNJeD7eOYJJvU6lOPPngfuc+csScHmfz2yTIpFq+gzn4cOHF8cZneS78GexT/h75/NIa31LBMIlEduA57g2kpQnVJC6euI7y88i4LLINu4e1lpS1O/z2fFsnaY56stZKHSKGpyFQqeowVkodIpUcx44cGBx7FO/5Pyu4cjlW9sqSNNpaE/qbbkOfBEv6iPXc9Qs2bOyqBf+zrTN/v37F8eu9bItDHltlkBM/eK6m3ZkOj7LFKE7gna4jud9WXtkGTZsAy+/pZ9dE66qR7N3QRs9w4a//Z3RVUY7vE3994tFfTkLhU5Rg7NQ6BQprWVUjdODjIJxmp6fdp82z6JeWvRpx44dk+uyyBwGu2e7dNPezJXi0/KkRXQrZDtWeTuS1mURJbzP25HPJrXP1u7xqBraQQrtkiWj160EAi+D74LJD1J75zlvN75Drr3kNjq15PvNkib4PHc7tWSb953WVhtSRQgVCic1anAWCp2iBmeh0ClW3gLQQQ5NTSVNtUi24BSvc/cGOTl1iCfWUlf5lD11FHWDZ7bw2a4XqXf9PmY1UOf41H6WmE77uc5ptg+Jh8Ox/CxLIlvkjO3qOrAFz9KhHXSD+Nq0fO+eLcT2pk3ucmE9va34bH8XTDineybbP2eVUDvphfXMMltKcxYKJzFqcBYKnSKltaQS/hkmJXBK2qJxPiXNMrPImQxZsjXpGSme0xQmIXsZvNbr2VprJ4uY8igmUv2WHJCm7e1txWeT/jqtJQ31dVlJybK2p8TwtiK1X3XtWH8X/O2uGoLPzrZS8MgitklGLenuySKySIfdxcX2zqRUC/XlLBQ6RQ3OQqFT1OAsFDpFqjmpnVwvknc71ya/Zna4T3lnGR/8Td3q+oVayTUWy3f7CeoSdwu1rnNQI7oGaq2f69dSf7rrgBrF9Qr1Ed9Ftr6tZ+3z2VlmC8t3bU2NxXr6ddSLrvtaIYCu2fiu/RxdKdn+OUS2baO3d+tdeN9kXXzN41UyVurLWSh0ihqchUKnSGktp+WdIpG2ODVpUR+PECIVzLIwsh2OWaZTsBad9PVWSQudbrTcA14O6alHzjBp3duxFX3i/842cArGepOuel1aa8JKU/rH9nAqT6qZuTpa2SVSO5tHmrZxtvYt4W6hbI1itj+v835FeeOyrbWAQLZjutczk0hz1JezUOgUNTgLhU6R0lpGwPiMFamJ7zrMz3uWJNxKWpWmNICUJpuR9TJaawg5ZcwiYkh9uGarNG0TSoCM7vmzWM9WIrA0rbfPPLeSi506kW5nO5Xz2Z6UTSrvfYJJA601bKWp3HCpwPbI3hmjpLxfsX189r01G+zls097e1Mi8ZwnTbCe3gZMcmihvpyFQqeowVkodIoanIVCp0g1J6fsXVfu3bt3cewZFNQl1FGe+Eqt4DqNWiTL1sgWUaKu4l4m27dvn1xHTevT8izDn826UW+4ntu6devi2BOlqfWoW90O2pitxUr49D3fSza1z3pl+tn1IvWdZ84Q2Y7PrWwnd6XwPq8/6+ZzFKwP7/PyaX8WTcW2cm3K6zzJ3t15y1BfzkKhU9TgLBQ6RUpruWVctqaNR72QIpASZGuDOhUk5eCx0wG6EbLoG7pBfEs32ut2kN54cnErSdspY0bP2K6kPh4FRPrrtJbls62ydWXdjmx9YYLt4eWzLqT5HrGTrQlLOs9zHjjOZzuFpvzwtmpRe3e5sIxs7ahsrWG2o0uiVVBfzkKhU9TgLBQ6RQ3OQqFTpJqTnNn1Ijm/c21y9NY231KeAM1QPNqRZa+4jqJOodZwjdLa6lySLr300uazqXGzZGg+z0MMqS2pS+j6kaZazPeEaYUf+l4p2TaIbDuW73oxc7O0wu18rWHC+1VLm7lriZkzWZ/wOQrqRdrlfbHlDpSmWpV9x7OR2P7Zms0t1JezUOgUNTgLhU6R0tpsypv0z8+RxmVJpTznO1bzs89pfqdIpImeWUC7mAXgFGPbtm1NG1vJuQ6nXUSWncB2ZLZD5p7KkpdJ8bLk3wy8z+tFuucUnb9bW2FI03fr59gnWBenu6zzli1bJufYPlmmEmmou1Jol9vIurEu2fpTLjGy5PE56stZKHSKGpyFQqdIaS2DtJ0ekKo5zeIMVivSx39nn/ksGonUx6NIWrNxHn2TzfhmkR2k77zPd1omTfTZydaWAD5DSEqdLavId+EUupUE7yC19LpkS2+2dnn2Z2XbJfBalx8E65m1lbcB32fr/Un5Dnut4HmXVbTRy1tl57L6chYKnaIGZ6HQKWpwFgqdItWcmSZk4vSq69a69qAG8qidTDsR1L4e3dOy0a9j3VxbU8dmW0ZQ92Sa0N0xrBvLyNam9XN79uxZHLd2bpambeXt3doi0d1MrTpL07ZrLaTl57y9qedYZ+9jfC9eBt97tg0i3Xeun7MsHbYr78uyhbytfOGBZagvZ6HQKWpwFgqdIqW1WXQPp9h9ypu0hVPXWUK1Bw23Eoh37do1uY7UxKkCaQbXX3WXDqmaR3nwt9/X2uXZk7n5e/PmzZNzpEVsN4+EIpV190ZrdzK3l/QyW2+Jayw5DWdd9u3bNznHdszWEMp2lKbNTNh2adOillIevcZIMbaBR6gx6sjfBW2hHU5/+dvXPF5l5/b6chYKnaIGZ6HQKWpwFgqdItWc2b4Y5OGtdVOlKSf3aW3+9vI5jU5N5fyf0/4+pb5jx47FMTWhh+TxPs9woF2eocHFy1imZyDQDtfdLdeH28gyva2opw8ePLg49rbKdA7LZ1tlSdketsn2oEvE65ztOM66UKd527eylvx3tit1tnAc4f27petdS2/atGlx7H0iy2JalHfcKwqFwsuCGpyFQqeIVdYyKRQKX3/Ul7NQ6BQ1OAuFTlGDs1DoFDU4C4VOUYOzUOgUNTgLhU7x/wAniEBL3NAY8AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show the image of the 0th item in the dataset\n",
    "fig, axs = plt.subplots()\n",
    "axs.imshow(sample_image.squeeze().numpy(), cmap='gray')\n",
    "axs.axis('off')\n",
    "axs.set_title('Sample image (Target label = {})'.format(sample_label))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sWOnZmNTdkJ"
   },
   "source": [
    "## 2 Model Architecture and Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bg59TFXHTdkL"
   },
   "source": [
    "### 2.1 SimpleNet Model\n",
    "\n",
    "The data is ready! Now we are preparing to move to the actual core of deep learning: the architecture. To get you started in this part, simply define a **2-layer** model in the class `SimpleNet` in `student_code.py`. Here by \"2 layers\" we mean **2 convolutional layers**, so you need to figure out the supporting utilities like ReLU, Max Pooling, and Fully Connected layers, and configure them with proper parameters to make the tensor flow.\n",
    "\n",
    "You may refer the image *simplenet.jpg* in the base folder for a sample network architecture (it's the architecture TAs used in their implementation and is sufficient to get you pass Part 1).\n",
    "\n",
    "**TODO 3**: Do the following in the class ```SimpleNet```:\n",
    "- Initialize ```self.cnn_layers```\n",
    "- Initialize ```self.fc_layers```\n",
    "- Write the forward function\n",
    "\n",
    "Leave the ```self.loss_criterion = None``` for now.\n",
    "\n",
    "**NOTE** : More hints in doc string of SimpleNet. Also, the doc of pytorch [here](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#) can be helpful. Feel free to explore more on the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jvVL-ap0BpFx",
    "outputId": "e8f5792b-d427-4dab-a3ca-29967cd89062"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing your SimpleNet architecture:  \u001b[32m\"Correct\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing your SimpleNet architecture: \", verify(test_simple_net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_YLUulTTdkX"
   },
   "source": [
    "### 2.2 Test the model structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNBpN4ofTdkZ"
   },
   "source": [
    "Let's see what out model's forward function produces for a sample input, and how it relates to classification. Pytorch's convolution and FC layers are initialized with random weights. So we should not expect any useful output without any training.\n",
    "\n",
    "We will use a data-point from the dataloader we have already created and run the model on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "9IW3f_SgTdkc"
   },
   "outputs": [],
   "source": [
    "# Initialize SimpleNet\n",
    "simple_model = SimpleNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "OyQDxArjTdkg"
   },
   "outputs": [],
   "source": [
    "# Get the 0th sample\n",
    "sample_image, sample_label = next(iter(image_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RQoag1zMf6e1",
    "outputId": "d7a14a69-351f-4ae8-9e1a-1029df2a616f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dimension:  torch.Size([1, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "sample_input = sample_image.unsqueeze(0)\n",
    "print(\"input dimension: \", sample_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "b0XEqQaNTdkw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64])\n",
      "torch.Size([1, 54080])\n"
     ]
    }
   ],
   "source": [
    "# run the image through the model\n",
    "sample_model_output = simple_model(sample_input).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2v9WUSlbTdkz",
    "outputId": "33ac8b90-2b9c-4e77-bdf6-f1c591521aca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0757, -0.0358, -0.0265,  0.0288, -0.0046,  0.0410,  0.0711, -0.1041,\n",
      "         -0.0040, -0.0742, -0.1032, -0.0071, -0.0169, -0.0947,  0.0376]])\n"
     ]
    }
   ],
   "source": [
    "print(sample_model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6r7z-reif6e2",
    "outputId": "88cd62f2-c12a-480c-a655-4414b767e2a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output dimension:  torch.Size([1, 15])\n"
     ]
    }
   ],
   "source": [
    "print(\"output dimension: \", sample_model_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCtiLVKKTdk3"
   },
   "source": [
    "We have a 15-dimensional tensor as output, but how does it relate to classification?\n",
    "\n",
    "We first convert the this tensor into a probability distribution over 15 classes by applying the [Softmax](https://en.wikipedia.org/wiki/Softmax_function) operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "_Ssu7W4jTdk4"
   },
   "outputs": [],
   "source": [
    "sample_probability_values = torch.nn.functional.softmax(sample_model_output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4BXWrwl0Tdk9",
    "outputId": "d3e610bd-64c1-4660-9be0-bdb041ffd6d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0728, 0.0652, 0.0658, 0.0695, 0.0672, 0.0704, 0.0725, 0.0609, 0.0673,\n",
      "         0.0627, 0.0609, 0.0671, 0.0664, 0.0614, 0.0701]])\n",
      "torch.Size([1, 15])\n"
     ]
    }
   ],
   "source": [
    "print(sample_probability_values)\n",
    "print(sample_probability_values.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTABRuGpTdlD"
   },
   "source": [
    "The prediction of the model will be the index where the probability distribution is the maximum. Convince yourself that the argmax-operation on *sample_model_output* is the same as the argmax-operation on *sample_probability_values*.\n",
    "\n",
    "**TODO 4:** Complete the ```predict_labels()``` function in ```student_code.py```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dpQzVaVf6e3"
   },
   "source": [
    "**Please provide the input and output dimensions of the SimpleNet in your report.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3j6LNUyTdlD"
   },
   "source": [
    "## 3 Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2mIs5sQTdlE"
   },
   "source": [
    "We have written a model which takes in a tensor for an image and produces a 15 dimensional output for it. We saw in the previous section on how the output relates to the prediction and probability distribution. But how do we quantify the performance of the model, and how do we use that quantification to form an objective function which we can minimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1esn_cCXTdlE"
   },
   "source": [
    "Ideally, we would want the probability function to have value 1 for the target *sample_label* and value 0 for the remaining class indices. To penalize the deviation between the desired probability distribution and the model-predicted distrtibution, we use the KL-divergence loss or the cross-entropy loss. Please refer to [this stackexchange post](https://datascience.stackexchange.com/questions/20296/cross-entropy-loss-explanation) for a good explanation and derivation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvSEgfOBTdlF"
   },
   "source": [
    "**TODO 5:** Assign a loss function to ```self.loss_criterion``` in ```__init__()``` function of the class ```SimpleNet``` you created in *TODO 3* . \n",
    "Note that we have not done a softmax operation in the model's forward function and choose the [appropriate loss function](https://pytorch.org/docs/stable/nn.html#loss-functions).\n",
    "\n",
    "**TODO 6:** Complete the ```compute_loss()``` function in ```student_code.py``` to use the model's loss criterion and compute the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "u9Y3TEv7TdlG"
   },
   "outputs": [],
   "source": [
    "simple_model = SimpleNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y2Lu0S_ETdlK",
    "outputId": "e40a3830-5c7e-4082-c026-bbcfc49d44f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNet(\n",
      "  (cnn_layers): Sequential(\n",
      "    (0): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ReLU()\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Linear(in_features=54080, out_features=500, bias=True)\n",
      "    (1): Linear(in_features=500, out_features=100, bias=True)\n",
      "    (2): Linear(in_features=100, out_features=15, bias=True)\n",
      "  )\n",
      "  (loss_criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(simple_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hbcEPDN0TdmF",
    "outputId": "b2f420a2-a02f-4102-d990-f464fcc13990"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing your model prediction:  \u001b[32m\"Correct\"\u001b[0m\n",
      "Testing your loss values:  \u001b[32m\"Correct\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing your model prediction: \", verify(test_predict_labels))\n",
    "print(\"Testing your loss values: \", verify(test_compute_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKQCj596TdlQ"
   },
   "source": [
    "## 4 Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CB0oPQtVTdlR"
   },
   "source": [
    "### 4.1 Manual gradient descent using Pytorch's autograd\n",
    "\n",
    "Till now, we have defined the model, and designed a loss function which is a proxy for *good* classification. We now have to optimize the weights of the network so that the loss function is minimized.\n",
    "\n",
    "Pytorch is a very useful library for deep learning because a lot of tensor operations and functions support the flow of gradients. This feature is called [autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html). This functionality lets us use gradient based optimization techniques like gradient descent without writing a lot of code.\n",
    "\n",
    "Let us first understand how we can access the gradients.\n",
    "\n",
    "### Define a model and a loss function\n",
    "Suppose we have a simple objective function that looks like:\n",
    "$$ L(w) =  w^2 - 10w + 25 $$\n",
    "\n",
    "This is a convex problem, and we know that the loss $L$ is minimized for $w=5$, and we can obtain this in closed form by setting the derivative wrt $w$ to $0$ and solving.\n",
    "\n",
    "But let us use gradient descent to obtain the solution in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "JLu-7PG1TdlS"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "\n",
    "def quadratic_loss(w: tensor) -> tensor:\n",
    "    assert w.shape==(1,)\n",
    "\n",
    "    # loss function\n",
    "    L = torch.pow(w, 2) - 10 * w + 25\n",
    "    \n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPVWOjo6TdlW"
   },
   "source": [
    "Let's compute the loss at w = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pbVokXpUTdlX",
    "outputId": "cc063b68-03f7-4055-df92-c529519ef123"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w=0.0000\tLoss=25.0000\n"
     ]
    }
   ],
   "source": [
    "w = tensor([0.0], requires_grad=True)\n",
    "\n",
    "loss = quadratic_loss(w)\n",
    "\n",
    "print('w={:.4f}\\tLoss={:.4f}'.format(w.detach().numpy().item(), loss.detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AabZ-6MnTdla"
   },
   "source": [
    "Now we can do a backward pass of the gradients to get the gradient of loss w.r.t w. Now we need to calculate the gradients with respect to the weights and biases using backprop. It will be very painful to do it manually, but thankfully, in PyTorch we've got it covered with autograd, which only needs a simple call of **.backward()** on our loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jcrIh9P3Tdlb",
    "outputId": "fdf710b7-3602-4413-84e8-6dedf2455473"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-10.])\n"
     ]
    }
   ],
   "source": [
    "# perform backward pass on loss (we need to retain graph here otherwise Pytorch will throw it away)\n",
    "loss.backward(retain_graph=True)\n",
    "\n",
    "print(w.grad.data)\n",
    "\n",
    "# explicitly zero the parameter gradients\n",
    "w.grad.zero_()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAPF2cBATdlf"
   },
   "source": [
    "Does this gradient match with the one you computed earlier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmd3JnxzTdli"
   },
   "source": [
    "With the gradients, we can update the weights and biases using gradient descent:\n",
    "$$w_{k+1}=w_{k} - \\alpha\\frac{\\partial L}{\\partial w_k}$$\n",
    "where $w$ is the parameter we are updating, $\\alpha$ is the learning rate, and $\\frac{\\partial L}{\\partial w_k}$ is the gradient at step $k$. You can learn more about gradient descent [here](https://en.wikipedia.org/wiki/Gradient_descent) and [here](https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "GIhMyeflTdlj"
   },
   "outputs": [],
   "source": [
    "# set learning rate\n",
    "lr = .03\n",
    "\n",
    "def gradientDescentStep(w: tensor, L: tensor, lr: float=1e-3) -> None:\n",
    "    '''\n",
    "    Take a step of the gradient descent\n",
    "    '''\n",
    "    \n",
    "    # manually zero out the gradient\n",
    "    w.grad.zero_()\n",
    "\n",
    "    # perform backward on loss (we need to retain graph here otherwise Pytorch will throw it away)\n",
    "    L.backward(retain_graph=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w -= lr * w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMEZ9eHuTdlm"
   },
   "source": [
    "Let's take one step of the gradient descent and check if the the loss value decreased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "0BuBQHZwTdln"
   },
   "outputs": [],
   "source": [
    "loss = quadratic_loss(w)\n",
    "\n",
    "gradientDescentStep(w, loss, lr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yIWP3Tv4Tdlq",
    "outputId": "afc232f5-c929-4750-d18c-f3a1397122b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w=0.3000\tLoss=22.0900\n"
     ]
    }
   ],
   "source": [
    "loss = quadratic_loss(w)\n",
    "print('w={:.4f}\\tLoss={:.4f}'.format(w.detach().numpy().item(), loss.detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Mb4IdocTdlv"
   },
   "source": [
    "Looks like it's been optimized!\n",
    "\n",
    "Now let's run a few more updates and see where we can get!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FTfmRlc0Tdlv",
    "outputId": "0867c0fe-5c7c-4538-e865-438cebc7bf0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10: w=2.3069\tLoss=7.2527\n",
      "Iteration 20: w=3.5495\tLoss=2.1040\n",
      "Iteration 30: w=4.2187\tLoss=0.6104\n",
      "Iteration 40: w=4.5792\tLoss=0.1771\n",
      "Iteration 50: w=4.7733\tLoss=0.0514\n",
      "Iteration 60: w=4.8779\tLoss=0.0149\n",
      "Iteration 70: w=4.9342\tLoss=0.0043\n",
      "Iteration 80: w=4.9646\tLoss=0.0013\n",
      "Iteration 90: w=4.9809\tLoss=0.0004\n",
      "Iteration 100: w=4.9897\tLoss=0.0001\n",
      "Iteration 110: w=4.9945\tLoss=0.0000\n",
      "Iteration 120: w=4.9970\tLoss=0.0000\n",
      "Iteration 130: w=4.9984\tLoss=0.0000\n",
      "Iteration 140: w=4.9991\tLoss=0.0000\n",
      "Iteration 150: w=4.9995\tLoss=0.0000\n",
      "Iteration 160: w=4.9997\tLoss=0.0000\n",
      "Iteration 170: w=4.9999\tLoss=0.0000\n",
      "Iteration 180: w=4.9999\tLoss=0.0000\n",
      "Iteration 190: w=5.0000\tLoss=-0.0000\n",
      "Iteration 200: w=5.0000\tLoss=0.0000\n",
      "\n",
      "optimization takes 0.069 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for i in range(200):\n",
    "    loss = quadratic_loss(w)\n",
    "    if not (i+1)%10:\n",
    "        print('Iteration {}: w={:.4f}\\tLoss={:.4f}'.format(\n",
    "            i+1, w.detach().numpy().item(), loss.detach().numpy().item()))\n",
    "        \n",
    "    gradientDescentStep(w, loss, lr) \n",
    "        \n",
    "print('\\noptimization takes %0.3f seconds'%(time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTFEo_JMTdlz"
   },
   "source": [
    "Seems that it's doing a great job training our model! The loss now has decreased significantly to a pretty small value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMtBy2JvTdl0"
   },
   "source": [
    "### 4.2 Optimization using Pytorch's gradient descent optimizer\n",
    "\n",
    "Now let's see how we can simplify this using the `torch.optim` package from PyTorch. You can see that using optimizer from `torch.optim` package can achieve the same results with a lot less code from our side. Also, there are many features available over the vanilla gradient descent. Let's use the Stochastic Gradient Descent (SGD) optimizer available in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YKb3ngxtTdl1",
    "outputId": "113a9b86-3084-4f5f-ab8f-ec9c4e5f3d91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10: w=2.3069\tLoss=8.2081\n",
      "Iteration 20: w=3.5495\tLoss=2.3812\n",
      "Iteration 30: w=4.2187\tLoss=0.6908\n",
      "Iteration 40: w=4.5792\tLoss=0.2004\n",
      "Iteration 50: w=4.7733\tLoss=0.0581\n",
      "Iteration 60: w=4.8779\tLoss=0.0169\n",
      "Iteration 70: w=4.9342\tLoss=0.0049\n",
      "Iteration 80: w=4.9646\tLoss=0.0014\n",
      "Iteration 90: w=4.9809\tLoss=0.0004\n",
      "Iteration 100: w=4.9897\tLoss=0.0001\n",
      "Iteration 110: w=4.9945\tLoss=0.0000\n",
      "Iteration 120: w=4.9970\tLoss=0.0000\n",
      "Iteration 130: w=4.9984\tLoss=0.0000\n",
      "Iteration 140: w=4.9991\tLoss=0.0000\n",
      "Iteration 150: w=4.9995\tLoss=0.0000\n",
      "Iteration 160: w=4.9997\tLoss=0.0000\n",
      "Iteration 170: w=4.9999\tLoss=0.0000\n",
      "Iteration 180: w=4.9999\tLoss=0.0000\n",
      "Iteration 190: w=5.0000\tLoss=0.0000\n",
      "Iteration 200: w=5.0000\tLoss=0.0000\n",
      "\n",
      "optimization takes 0.063 seconds\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "# define parameters we want to optimize\n",
    "w = tensor([0.0], requires_grad=True)\n",
    "\n",
    "optimizer = SGD([w], lr=lr)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(200):\n",
    "    loss = quadratic_loss(w)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if not (i+1)%10:\n",
    "        print('Iteration {}: w={:.4f}\\tLoss={:.4f}'.format(\n",
    "            i+1, w.detach().numpy().item(), loss.detach().numpy().item()))\n",
    "        \n",
    "print('\\noptimization takes %0.3f seconds'%(time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IsnddUCTdl5"
   },
   "source": [
    "### 4.3 Setting up the optimizer for SimpleNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3pyzj8KTdl_"
   },
   "source": [
    "We will now set up a utility function to define an optimizer on the loss for a model.\n",
    "\n",
    "**TODO 7:** complete the ```get_optimizer()``` function in ```student_code.py```. The helper function accepts three basic configurations as defined below. Any other configuration is optional. *SGD* optimizer type should be supported, anything else is optional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxeVYy1bTdl6"
   },
   "source": [
    "We need to define parameters such as - the optimizer type we want to use ( sgd, adam, etc. ), learning rate, weight decay, etc.\n",
    "\n",
    "Here, we have initialized them to *placeholder* values for now. You'll train the model with these values and it will be bad. Then you can come back here and tune the parameters. We will revisit this part again later to tune these inorder to improve our classification. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "V2cwtK5PBpF7"
   },
   "outputs": [],
   "source": [
    "# TODO: add a decent initial setting and tune from there\n",
    "optimizer_config = {\n",
    "  \"optimizer_type\": \"adam\",\n",
    "  \"lr\": 1e-1,\n",
    "  \"weight_decay\": 1e-1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "P0CrYZa4BpGE"
   },
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(simple_model, optimizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(optimizer is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGUGDaUxf6e9"
   },
   "source": [
    "**Please briefly explain how an optimizer works in the report after going through this section.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b8cjmrSTdmK"
   },
   "source": [
    "## 5 Training SimpleNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBrJnj2tTdmL"
   },
   "source": [
    "Great! We have completed all the components required to train the our model. Let's pass in the model architecture, optimizer, transforms for both the training and testing datasets into the trainer, and proceed to the next cell to train it. If you have implemented everything correctly, you should be seeing a decreasing loss value.\n",
    "\n",
    "We will use the ``Trainer`` class from ```trainer.py``` file which contains all the essential functions for training, plotting the accuracies. Make sure to understand it. \n",
    "\n",
    "**Note** in this project, we will be using the test set as the validation set (i.e. using it to guide our decisions about models and hyperparameters while training. In actual practice, you would not interact with the test set until reporting the final results. Oftentimes, you would not *know* the test set much less interacting with it.\n",
    "\n",
    "**Note** that your CPU should be sufficient to handle the training process for all networks in this project, and the following training cells will take less than 5 minutes; you may also want to decrease the value for `num_epochs` and quickly experiment with your parameters. The default value of **20** is good enough to get you around the threshold for this Part, and you are free to increase it a bit and adjust other parameters in this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "UiGOvPJfBpGO"
   },
   "outputs": [],
   "source": [
    "# re-init the model so that the weights are all random\n",
    "simple_model = SimpleNet()\n",
    "optimizer = get_optimizer(simple_model, optimizer_config)\n",
    "input_size = (64,64)\n",
    "trainer = Trainer(data_dir=data_base_path, \n",
    "                  model = simple_model,\n",
    "                  optimizer = optimizer,\n",
    "                  model_dir = os.path.join(model_base_path, 'simple_net'),\n",
    "                  train_data_transforms = get_fundamental_transforms(input_size, dataset_mean, dataset_std),\n",
    "                  test_data_transforms = get_fundamental_transforms(input_size, dataset_mean, dataset_std),\n",
    "                  batch_size = 32,\n",
    "                  load_from_disk = False,\n",
    "                  cuda = is_cuda\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "paNLyU5cBpGX",
    "outputId": "6041400f-4e58-41fe-ee27-a46448e5c491"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Training Loss:2.7054, Validation Loss:2.7127\n",
      "Epoch:1, Training Loss:14.5174, Validation Loss:17.0099\n",
      "Epoch:2, Training Loss:46.7803, Validation Loss:56.6402\n",
      "Epoch:3, Training Loss:24.4523, Validation Loss:28.7055\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mx/yclsyjl13_77q65fwgpbmldr0000gn/T/ipykernel_65348/3051314158.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msimple_net_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msimple_net_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The training time taken for simple net is {:.9f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimple_net_end\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msimple_net_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/cs4476/cv_proj3_release/cv_proj3_code/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cv_proj3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cv_proj3/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                    )\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cv_proj3/lib/python3.7/site-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "simple_net_start = time.time()\n",
    "trainer.train(num_epochs=20)\n",
    "simple_net_end = time.time()\n",
    "print(\"The training time taken for simple net is {:.9f}\".format(simple_net_end-simple_net_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jv1T8xv2TdmR"
   },
   "source": [
    "Now let's have your model predict on some examples and see how well it performs qualitatively. You should try the following cell multiple times to understand whats happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "thCGob3JTdmR",
    "outputId": "fe52526e-cbf3-41b8-b811-0c9587ce65b3"
   },
   "outputs": [],
   "source": [
    "# visualize train split\n",
    "print(\"Examples from train split:\")\n",
    "visualize(simple_model, 'train', get_fundamental_transforms(input_size, dataset_mean, dataset_std), data_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "QQbkZhjlTdmU",
    "outputId": "126aeca4-8480-4b0d-8af8-06efed12ec18"
   },
   "outputs": [],
   "source": [
    "# visualize test split\n",
    "print(\"Examples from test split:\")\n",
    "visualize(simple_model, 'test', get_fundamental_transforms(input_size, dataset_mean, dataset_std), data_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "z0b_WwJhBpGf",
    "outputId": "69d5c609-1fb9-47ed-a209-32a30c6d745e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.plot_loss_history()\n",
    "trainer.plot_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8epn0IBmBpGn",
    "outputId": "f4b6e67c-e964-4710-cbe5-b8118a47d371",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_accuracy = trainer.train_accuracy_history[-1]\n",
    "validation_accuracy = trainer.validation_accuracy_history[-1]\n",
    "print('Train Accuracy = {:.4f}; Validation Accuracy = {:.4f}'.format(train_accuracy, validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CQKKbNc2haCZ",
    "outputId": "bc893bce-c847-4eb3-a52c-6439a1e96827"
   },
   "outputs": [],
   "source": [
    "print('Testing simple net weights saved: ', verify(test_simple_net_checkpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWNCMmRzTdmc"
   },
   "source": [
    "After you have finished the training process, now plot out the loss and accuracy history. You can also check out the final accuracy for both training and validation data. Copy the accuracy plots and values onto the report, and answer the questions there. \n",
    "\n",
    "**TODO 8:** Obtain a **45%** validation accuracy to receive full credits for Part 1. You can go back to **Section 4.3** where we initialized the parameters for the optimizer. These are crucial to increase the performance of the model. So, tune your paramters for optimization using the following tips:\n",
    "\n",
    "**Tips**:\n",
    "1. If the loss decreases very slowly, try increasing the value of the lr (learning rate).\n",
    "2. Initially keep the value of weight decay (L2-regulization) very low.\n",
    "3. Try to first adjust lr in multiples of 3 initially. When you are close to reasonable performance, do a more granular adjustment.\n",
    "4. If you want to increase the validation accuracy by a little bit, try increasing the weight_decay to prevent overfitting. Do not use tricks from subsequent sections just yet.\n",
    "\n",
    "If you still need to tweak the model architecture, you are free to do so. But remember complex models will require more time to train, and TAs could achieve ~50% accuracy with the described model.\n",
    "\n",
    "Make note of the hyperparameters that you finally arrived at. You will be asked to present them in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amh1wxlJBpHj"
   },
   "source": [
    "## 6 Pretrained AlexNet\n",
    "You can see that with our basic 2-layer SimpleNet, we were able to classify the images. However, we are not satisfied with its performance yet. We could add other techniques like Dropout which we defer to Extra Credit. \n",
    "\n",
    "We are going to take a different approach at the task now. Our model, in the end, is still a 2-layer SimpleNet and it might be capable of capturing some features, but it could be improved a lot if we go **deeper**. In this part, we are going to see the power of a famous model: AlexNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7hE1vidBpGt"
   },
   "source": [
    "### 6.1 Data Augmentation with Jitter, Random Flip, and Normalization\n",
    "\n",
    "Before diving into using Alexnet, we could do something to increase the amount of data samples we have. Since, having more data is always better.\n",
    "\n",
    "One common and simple technique to achieve it is to **augment** it. Firstly, let's \"jitter\" it; secondly, we will use the fact that when you *mirror* an image of a kitchen, you can tell that the mirrored image is still a kitchen. \n",
    "\n",
    "**TODO 9:** finish the `get_data_augmentation_transforms()` function in `student_code.py`: you should first copy over the transforms you used in the existing fundamental transform implementation into this function, and then insert a couple of other transforms which help you do the above adjustment. This is because, we can pass only one augmentation function at a time to the trainer.\n",
    "\n",
    "You are free to experiment with different kinds of augmentation techniques by adding new techniques or replacing existing techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_transforms = get_data_augmentation_transforms((64, 64), dataset_mean, dataset_std )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(augmentation_transforms.transforms) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMfz9HrpTdnO"
   },
   "source": [
    "### 6.2 Building AlexNet\n",
    "PyTorch has provided us with pre-trained models like AlexNet, so what you want to do is to load the model first, and then adjust some of the layers such that it fits with our own dataset, instead of outputing scores to 1000 classes from the original AlexNet model.\n",
    "\n",
    "There are many reasons for using pre-trained weights instead of training AlexNet from scratch:\n",
    "1. We have a really small dataset ( even after augmenting ). Alexnet has millions of parameters and will definitely overfit. The pre-trained alexnet will have access to millions of training images.\n",
    "2. We save a lot of computation.\n",
    "\n",
    "**TODO 10:** Modify class `MyAlexNet` in `student_code.py`, for the below cell to work. Copy the network architecture and weights of all but the last fc layers from the pretrained network.(Think why all but last fc !)\n",
    "\n",
    "After you have defined the correct architecture of the model, make some tweaks to the existing layers: **freeze** the **convolutional** layers and first 2 **linear** layers so we don't update the weights of them. More instructions are in the docstring.\n",
    "\n",
    "Note that you are allowed to add more layers/unfreeze more layers if you see fit.\n",
    "\n",
    "**NOTE** : It will be beneficial to look at the schematic of the AlexNet before modifying the layers. Understand what layers to freeze and their indices. Use the below cell to view the network before and after your modifications. Of course, you've to load the pre-trained model in `MyAlexNet` for the below cell to work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545,
     "referenced_widgets": [
      "4aef299be5ce45979ec0c3d00db62d56",
      "c93998cd65084de8aadf98fbe935914c",
      "8aed1f2e022444c080dcdaebe6909feb",
      "8a8b0621eb074119abc5b2e21992caa6",
      "3121c70e576640bfa7d982ffd412ce5b",
      "394611813d79423eac598c776f1f6b5e",
      "486affe7483d4a2c9860a80a371b37a8",
      "317a33ad1835417f8cbabb84475e33ce",
      "8fef908f023d49c9a89bb24514ceb5c6",
      "e1adb4ae8ad340ebb8747ca154c1941d",
      "0f98bc466fa24ec1b989e72ce7f278a4"
     ]
    },
    "id": "CBoLgRrlBpHl",
    "outputId": "db8b2bad-434c-4309-b7e6-5002f756b601"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyAlexNet(\n",
      "  (cnn_layers): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=15, bias=True)\n",
      "  )\n",
      "  (loss_criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "my_alexnet = MyAlexNet()\n",
    "print(my_alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DmXFp9NdTdnO",
    "outputId": "b5cf3150-6aa9-48c0-8eec-8891962c3787"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing your AlexNet architecture:  \u001b[32m\"Correct\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing your AlexNet architecture: \", verify(test_my_alexnet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Hi05e4Dkb2u"
   },
   "source": [
    "### 6.3 Training AlexNet\n",
    "We will now train the `AlexNet`.\n",
    "\n",
    "As done before, we initialize parameters for optimizer for AlexNet which you need to tune later to improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "J6AYkHAgBpHw"
   },
   "outputs": [],
   "source": [
    "# TODO: add a decent initial setting and tune from there\n",
    "optimizer_config = {\n",
    "  \"optimizer_type\": \"adam\",\n",
    "  \"lr\": 1e-1,\n",
    "  \"weight_decay\": 1e-1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "DtCIaTMmBpIK"
   },
   "outputs": [],
   "source": [
    "my_alexnet = MyAlexNet()\n",
    "optimizer = get_optimizer(my_alexnet, optimizer_config)\n",
    "input_size = (224, 224)\n",
    "\n",
    "trainer = Trainer(data_dir=data_base_path, \n",
    "                  model = my_alexnet,\n",
    "                  optimizer = optimizer,\n",
    "                  model_dir = os.path.join(model_base_path, 'alexnet'),\n",
    "                  train_data_transforms = get_data_augmentation_transforms(input_size, dataset_mean, dataset_std),\n",
    "                  test_data_transforms = get_fundamental_transforms(input_size, dataset_mean, dataset_std),\n",
    "                  batch_size = 32,\n",
    "                  load_from_disk = False,\n",
    "                  cuda = is_cuda\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOeaVrlZTdnY"
   },
   "source": [
    "The following training cell will take roughly 20 minutes or slightly more using CPU (but possibly under 5 minute using GPU depending on the batch size; the TAs got it within 3 minutes on colab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CAcncwLPBpIQ",
    "outputId": "d835b9fe-ac95-49cd-dd75-ca58c42085e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Training Loss:2.7875, Validation Loss:2.8085\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mx/yclsyjl13_77q65fwgpbmldr0000gn/T/ipykernel_65348/3194579454.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/cs4476/cv_proj3_release/cv_proj3_code/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cv_proj3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cv_proj3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ax5cpYmZTdna"
   },
   "source": [
    "Now let's have your model predict on some examples and see how well it performs qualitatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "rc_IeNR9Tdnb",
    "outputId": "3d19603d-084b-4c9b-f0b7-e67997272569"
   },
   "outputs": [],
   "source": [
    "# # visualize train split\n",
    "print(\"Examples from train split:\")\n",
    "visualize(my_alexnet, 'train', get_fundamental_transforms(input_size, dataset_mean, dataset_std), data_base_path=data_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "IMxmCz4WTdnc",
    "outputId": "9322e059-42c9-4a0d-9ee8-9f97760008e0"
   },
   "outputs": [],
   "source": [
    "# # visualize test split\n",
    "print(\"Examples from test split:\")\n",
    "visualize(my_alexnet, 'test', get_fundamental_transforms(input_size, dataset_mean, dataset_std), data_base_path=data_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "Cimj95G_BpIU",
    "outputId": "9be612f9-5145-4251-f81b-34aff68a413e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.plot_loss_history()\n",
    "trainer.plot_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qMI3CdEuBpIb",
    "outputId": "46e9ec03-c095-497c-923c-c5756513b0e3"
   },
   "outputs": [],
   "source": [
    "train_accuracy = trainer.train_accuracy_history[-1]\n",
    "validation_accuracy = trainer.validation_accuracy_history[-1]\n",
    "print('Train Accuracy = {:.4f}; Validation Accuracy = {:.4f}'.format(train_accuracy, validation_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6DN_wGXTdni"
   },
   "source": [
    "**TODO 11**: Similar to what we've done in the SimpleNet section, you are required to pass a threshold of **85%** for this part. For that, tune the hyperparameters. \n",
    "\n",
    "Copy the plots and values onto the report, present the final hyperparameters, and answer questions in the report accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOQp5yLN__9d"
   },
   "source": [
    "## Extra Credits : \n",
    "***This part is optional for undergraduate students (4476), but required for graduate students (6476)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06cXE2wkTdmg"
   },
   "source": [
    "## Extra Credit 1: Dropout\n",
    "\n",
    "### Overfitting\n",
    "We have obtained a 45% accuracy on the validation data with a SimpleNet; If you observe, it has good training accuracy (more than 90%, if you have implemented everything correctly) but not so great validation accuracy.\n",
    "\n",
    "Our final accuracies for training and validation data differ a lot from each other, which indicates that the model we defined **fits too well with the training data, but is unable to generalize well on data it has not trained on**: this is often regarded as **overfitting** where the models fits the training data a little too well. We have some techniques to tackle with it: adjusting both data and model.\n",
    "\n",
    "In **Section 6.1** we have seen how to adjust the data by using data augmentation. Now we will see how to adjust the model itself.\n",
    "\n",
    "\"Dropout\" is a technique commonly used to regularize the network. It randomly turns off the connection between neurons inside the network and prevent the network from relying too much on a specific neuron. \n",
    "\n",
    "**TODO EC1.1:** finish the class `SimpleNetDropout` in `student_code.py`. It should be same as your previous SimpleNet model, except the dropout layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "adEPCL3QTdmi",
    "outputId": "7aeb8152-d170-437f-e9be-e0944d6fabfc"
   },
   "outputs": [],
   "source": [
    "print(\"Testing your SimpleNetDropout architecture: \", verify(test_simple_net_dropout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nlgm5eM-BpGz",
    "outputId": "513be8e7-5221-485c-8d23-561611de81d5"
   },
   "outputs": [],
   "source": [
    "simple_model_dropout = SimpleNetDropout()\n",
    "print(simple_model_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmmpMDRBTdmm"
   },
   "source": [
    "Similar to the previous parts, **initialize the following cell with proper values for learning rate and weight decay** for SimpleNetDropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "btKIvIrdBpG5"
   },
   "outputs": [],
   "source": [
    "# TODO: add a decent initial setting and tune from there\n",
    "optimizer_config = {\n",
    "  \"optimizer_type\": \"adam\",\n",
    "  \"lr\": 1e-1,\n",
    "  \"weight_decay\": 1e-1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ExoLylurBpHH"
   },
   "outputs": [],
   "source": [
    "simple_model_dropout = SimpleNetDropout()\n",
    "optimizer = get_optimizer(simple_model_dropout, optimizer_config)\n",
    "input_size = (64,64)\n",
    "trainer = Trainer(data_dir=data_base_path, \n",
    "                  model = simple_model_dropout,\n",
    "                  optimizer = optimizer,\n",
    "                  model_dir = os.path.join(model_base_path, 'simple_net_dropout'),\n",
    "                  train_data_transforms = get_data_augmentation_transforms(input_size, dataset_mean, dataset_std),\n",
    "                  test_data_transforms = get_fundamental_transforms(input_size, dataset_mean, dataset_std),\n",
    "                  batch_size = 32,\n",
    "                  load_from_disk = False,\n",
    "                  cuda = is_cuda\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zq2__3ZQTdmt"
   },
   "source": [
    "The following cell will take longer than that of SimpleNet, as now we have more data (and more variability), and the model is slightly more complicated than before as well; however, it should finish within 10~15 minutes anyway, and the default `num_epochs` of **30** is also good enough as a starting point for you to pass this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ljUl4UnBpHN",
    "outputId": "15512dbe-64cb-41ed-977c-a6c5df934655",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train(num_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3yOROVGTdmy"
   },
   "source": [
    "Now let's have your model predict on some examples and see how well it performs qualitatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "732LGRJCftzL",
    "outputId": "29a6d1e1-ad3c-4fad-88ed-c55c747d604a"
   },
   "outputs": [],
   "source": [
    "# # visualize train split\n",
    "print(\"Examples from train split:\")\n",
    "visualize(simple_model_dropout, 'train', get_fundamental_transforms(input_size, dataset_mean, dataset_std), data_base_path=data_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "Jum7rI93Tdm0",
    "outputId": "8da14be7-a9c5-4521-d7a9-f79eaf9185e0"
   },
   "outputs": [],
   "source": [
    "# # visualize test split\n",
    "print(\"Examples from test split:\")\n",
    "visualize(simple_model_dropout, 'test', get_fundamental_transforms(input_size, dataset_mean, dataset_std), data_base_path=data_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "Gdh9AvHIBpHW",
    "outputId": "221994df-5b60-427f-9314-3c5b81be9f53",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.plot_loss_history()\n",
    "trainer.plot_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6SLuc3zmBpHd",
    "outputId": "882e46d8-c754-4076-8090-0aca3de3d75e"
   },
   "outputs": [],
   "source": [
    "train_accuracy = trainer.train_accuracy_history[-1]\n",
    "validation_accuracy = trainer.validation_accuracy_history[-1]\n",
    "print('Train Accuracy = {:.4f}; Validation Accuracy = {:.4f}'.format(train_accuracy, validation_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBLf3n6aTdnJ"
   },
   "source": [
    "Similar to the previous part, now plot out the loss and accuracy history. Also copy the plots onto the report, and answer the questions accordingly.\n",
    "\n",
    "**TODO EC1.2:** Achieve **52%** validation accuracy for full credits for this part. Tune hyperparameters.\n",
    "\n",
    "Compare the training and valdiation accuracies you've achieved with the SimpleNet without any data augmentation and Dropout.\n",
    "Copy the plots and values onto the report, present the final hyperparameters, and answer questions in the report accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJNZaFSqhaD5"
   },
   "source": [
    "## Extra Credit 2: Model Space/Compute improvement\n",
    "\n",
    "We have created the AlexNet model which performs really well. But, can we deploy on a robot which does not have access to powerful GPUs and memory ? What if we want to make the model simpler and faster for inference. One option will be to use SimpleNet or a variant of it. But that has very low accuracy compared to AlexNet.\n",
    "\n",
    "Can we utilize the AlexNet model which we have learnt and improve the memory/compute usage? This is an active research area with [several possible options](https://arxiv.org/pdf/1710.09282.pdf).\n",
    "\n",
    "In this part, we will try quantizing our filter weights from 32-bit floating point to 8-bit integer. As you can guess, all the weights in the layers (in convolution and fully connected layers) will have a 75% reduction from this simple switch.\n",
    "\n",
    "What about compute time? The first benefit is directly from lower memory foot-print. We can transfer more weights in the fixed memory bandwith and the cache can hold more weights too. Some CPU architectures might have INT8 computation units which can offer more speedup.\n",
    "\n",
    "We might lose some accuracy when we sacrifice precision. Doing the fp32->int8 conversion directly is a bad choice. \n",
    "Suppose an activation is in a small range of $[0.1, 0.2]$. We cannot just round the weights as the activation might be the same for all the inputs. Hence we need to quantize based on the statistics of the activations.\n",
    "\n",
    "This is a very brief introduction, and we expect you to read material online to complete this section.\n",
    "[Pytorch's Introduction to Quantization](https://pytorch.org/blog/introduction-to-quantization-on-pytorch/) is a good starting point. We used the post-training static quantization scheme and got good performance. You are free to use any quantization scheme as long as it falls within one of the buckets of the rubric. **Please use the quantization modules provided by Pytorch as described in Pytorch's Introduction to Quantization.**\n",
    "\n",
    "**TODO EC2.1**: Write the `forward` function in class `MyAlexNetQuantized`. Do remember that the input and the output of the function will not be quantized and you need to pass them through a layer for quantization/dequantization.\n",
    "\n",
    "\n",
    "**TODO EC2.2**: Write the `quantize_model` function in `student_code.py`.\n",
    "\n",
    "**Grading**: There are no unit tests for this portion. You will be required to write your idea, paste code snippets, and results in the report. The rubric for this section is a bit complicated and it is recommended to review it.\n",
    "\n",
    "Please note that not all possible buckets may be possible to achieve using simple post-training static quantization scheme. You can obtain a decent performance here and come back later if you are inclined. Make sure you understand what you are doing as points will be alloted for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3W2xS4ShaD5"
   },
   "outputs": [],
   "source": [
    "from cv_proj3_code.student_code import quantize_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLY3-92zhaD8"
   },
   "outputs": [],
   "source": [
    "original_model = MyAlexNet().to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7RCEz4vYhaD_"
   },
   "outputs": [],
   "source": [
    "input_size = (224, 224)\n",
    "alexnet_trainer = Trainer(data_dir=data_base_path, \n",
    "                  model = original_model,\n",
    "                  optimizer = get_optimizer(original_model, optimizer_config),\n",
    "                  model_dir = os.path.join(model_base_path, 'alexnet'),\n",
    "                  train_data_transforms = get_data_augmentation_transforms(input_size, dataset_mean, dataset_std),\n",
    "                  test_data_transforms = get_fundamental_transforms(input_size, dataset_mean, dataset_std),\n",
    "                  batch_size = 32,\n",
    "                  load_from_disk = True,\n",
    "                  cuda = False\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wb66xBschaEB"
   },
   "outputs": [],
   "source": [
    "image_loader = ImageLoader(data_base_path, \n",
    "                           split='train', \n",
    "                           transform=get_fundamental_transforms(input_size, dataset_mean, dataset_std)\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MIgwwiLshaED",
    "outputId": "1c28d1d0-be60-4a04-8692-b9278ba3e146"
   },
   "outputs": [],
   "source": [
    "quantized_alexnet = quantize_model(original_model, image_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X7ELeeAXhaEF"
   },
   "outputs": [],
   "source": [
    "quantized_alexnet_trainer = Trainer(data_dir=data_base_path, \n",
    "                  model = quantized_alexnet,\n",
    "                  optimizer = get_optimizer(original_model, optimizer_config),\n",
    "                  model_dir = os.path.join(model_base_path, 'quantized_alexnet'),\n",
    "                  train_data_transforms = get_data_augmentation_transforms(input_size, dataset_mean, dataset_std),\n",
    "                  test_data_transforms = get_fundamental_transforms(input_size, dataset_mean, dataset_std),\n",
    "                  batch_size = 32,\n",
    "                  load_from_disk = False,\n",
    "                  cuda = False\n",
    "                 )\n",
    "\n",
    "quantized_alexnet_trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTRFv4D3haEH"
   },
   "source": [
    "### Size comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssNmgOCNhaEH"
   },
   "outputs": [],
   "source": [
    "quantized_alexnet_size = os.path.getsize(os.path.join(model_base_path, 'quantized_alexnet', 'checkpoint.pt'))/1e6\n",
    "alexnet_size = os.path.getsize(os.path.join(model_base_path, 'alexnet', 'checkpoint.pt'))/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rn_vbAMohaEJ",
    "outputId": "b98988ad-ea5b-4790-a0ec-e71916f89169"
   },
   "outputs": [],
   "source": [
    "print('Original Alexnet Size: {:.2f}'.format(alexnet_size))\n",
    "print('Quantized Alexnet Size: {:.2f}'.format(quantized_alexnet_size))\n",
    "print('Percentage reduction: {:.2f}%'.format(100-100*quantized_alexnet_size/alexnet_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SY7N9paphaEM"
   },
   "source": [
    "Now let's have your model predict on some examples and see how well it performs qualitatively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lo7-Hddnlou"
   },
   "source": [
    "### Processing Time comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u83cw0MxhaEP"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def time_forward_pass(model: torch.nn.Module, input_tensor: torch.tensor) -> float:\n",
    "    model.eval()\n",
    "    start = time.time()\n",
    "    for _ in range(300):\n",
    "        model(input_tensor)\n",
    "    end = time.time()\n",
    "    \n",
    "    return (end-start)/300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TIY2v2s5haEY"
   },
   "outputs": [],
   "source": [
    "sample_input, _ = next(iter(image_loader))\n",
    "sample_input = sample_input.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4pAK68ohaEb"
   },
   "outputs": [],
   "source": [
    "quantized_alexnet_time = time_forward_pass(quantized_alexnet, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ldb6amFchaEc"
   },
   "outputs": [],
   "source": [
    "alexnet_time = time_forward_pass(original_model, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PJUdiCrehaEd",
    "outputId": "273df9c4-ae85-4de2-f0df-1899a412db48"
   },
   "outputs": [],
   "source": [
    "print('Original Alexnet Time: {:.2f}'.format(alexnet_time))\n",
    "print('Quantized Alexnet Time: {:.2f}'.format(quantized_alexnet_time))\n",
    "print('Percentage reduction: {:.2f}%'.format(100-100*quantized_alexnet_time/alexnet_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "teAdeSYmhaEf"
   },
   "outputs": [],
   "source": [
    "_, alexnet_accuracy = alexnet_trainer.evaluate()\n",
    "_, quantized_alexnet_accuracy = quantized_alexnet_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WKN3q0vMhaEi",
    "outputId": "3fc7cb57-d635-4ec0-ac33-d990e43fe46f"
   },
   "outputs": [],
   "source": [
    "print('Original Alexnet Accuracy: {:.2f}'.format(alexnet_accuracy))\n",
    "print('Quantized Alexnet Accuracy: {:.2f}'.format(quantized_alexnet_accuracy))\n",
    "print('Percentage reduction: {:.2f}%'.format(100-100*quantized_alexnet_accuracy/alexnet_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaopDcToTdnj"
   },
   "source": [
    "## Code testing\n",
    "We have provided a set of tests for you to evaluate your implementation. We have included tests inside ```proj3.ipynb``` so you can check your progress as you implement each section. At the end, you should call the tests from the terminal using the command ```pytest cv_proj3_code/cv_proj3_unit_tests/```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIfkkgSahaE8"
   },
   "source": [
    "## Submission\n",
    "\n",
    "This is very important as you will lose 5 points for every time you do not follow the instructions.\n",
    "\n",
    "Don't install any additional packages inside the conda environment. The TAs will use the same environment as defined in the config files we provide you, so anything that's not in there by default will probably cause your code to break during grading. Don't use absolute paths in your code or your code will break. Use relative paths like the starter code already does. Failure to follow any of these instructions will lead to point deductions. Create the zip file using ```python zip_submission.py --gt_username <your_gt_username>``` (it will zip up the appropriate directories/files for you!) and hand it through Gradescope. Remember to submit your report as a PDF to Gradescope as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElYsyIdUkb21"
   },
   "source": [
    "## TODO Checklist\n",
    "\n",
    "### Required For All\n",
    "* **TODO 1** : Understand the class ```ImageLoader``` and answer questions in report.\n",
    "* **TODO 2** : complete the function ```get_fundamental_transforms()```\n",
    "* **TODO 3** : complete the class ```SimpleNet```\n",
    "* **TODO 4** : complete the function ```predict_labels()```\n",
    "* **TODO 5** : Assign appropriate loss function to  ```self.loss_criterion``` in ```SimpleNet```\n",
    "* **TODO 6** : complete the function ```compute_loss()```\n",
    "* **TODO 7** : complete the function ```get_optimizer()```\n",
    "* **TODO 8** : tune the hyperparameters for training ```SimpleNet``` to get validation accuracy of **>45%**\n",
    "* **TODO 9** : complete the function ```get_data_augmentation_transforms()```\n",
    "* **TODO 10** : Modify the class ```MyAlexNet```\n",
    "* **TODO 11** : tune the hyperparameters for training ```MyAlexNet``` to get validation accuracy of **>85%**\n",
    "\n",
    "### Extra Credit\n",
    "* **TODO EC1.1** : finish the class `SimpleNetDropout` \n",
    "* **TODO EC1.2** : tune the hyperparameters for training `SimpleNetDropout` to get validation accuracy of **>52%**\n",
    "* **TODO EC2.1** : Write the ```forward``` function in class ```MyAlexNetQuantized```\n",
    "* **TODO EC2.2** : Write the ```quantize_model``` function in ```student_code.py```.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwzSFdI-haE8"
   },
   "source": [
    "## Rubric\n",
    "The distribution of points for code and report are mentioned in gradescope and report files.\n",
    "\n",
    "**Rubric for EC2:**\n",
    "- 3 points for describing the strategy and explaining the main ideas in the report.\n",
    "- 3 If you achieve >50% reduction in size, >10% reduction in time, and <5% reduction in accuracy. That's a resonable output if you're using the quantization module of Pytorch properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMUaxM3GhaE8"
   },
   "source": [
    "## Credit\n",
    "\n",
    "Assignment developed by Sarath Mutnuru and Lixing Liu based on a the original assignment by Ayush Baid, Haoxin Ma, Jing Wu, Cusuh Ham, Jonathan Balloch, Shenhao Jiang, Frank Dellaert, and James Hays."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "proj3.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "43a6b7e3d4ae5499fd7e2bbb219e339a812f6f1dcf375ff2fedb650f9732f8f9"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0f98bc466fa24ec1b989e72ce7f278a4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3121c70e576640bfa7d982ffd412ce5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f98bc466fa24ec1b989e72ce7f278a4",
      "placeholder": "​",
      "style": "IPY_MODEL_e1adb4ae8ad340ebb8747ca154c1941d",
      "value": " 233M/233M [00:10&lt;00:00, 23.4MB/s]"
     }
    },
    "317a33ad1835417f8cbabb84475e33ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "394611813d79423eac598c776f1f6b5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "486affe7483d4a2c9860a80a371b37a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4aef299be5ce45979ec0c3d00db62d56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8aed1f2e022444c080dcdaebe6909feb",
       "IPY_MODEL_8a8b0621eb074119abc5b2e21992caa6",
       "IPY_MODEL_3121c70e576640bfa7d982ffd412ce5b"
      ],
      "layout": "IPY_MODEL_c93998cd65084de8aadf98fbe935914c"
     }
    },
    "8a8b0621eb074119abc5b2e21992caa6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8fef908f023d49c9a89bb24514ceb5c6",
      "max": 244408911,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_317a33ad1835417f8cbabb84475e33ce",
      "value": 244408911
     }
    },
    "8aed1f2e022444c080dcdaebe6909feb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_486affe7483d4a2c9860a80a371b37a8",
      "placeholder": "​",
      "style": "IPY_MODEL_394611813d79423eac598c776f1f6b5e",
      "value": "100%"
     }
    },
    "8fef908f023d49c9a89bb24514ceb5c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c93998cd65084de8aadf98fbe935914c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e1adb4ae8ad340ebb8747ca154c1941d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
